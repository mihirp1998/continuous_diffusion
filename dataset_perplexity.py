from datasets import load_dataset
from transformers import GPT2Config, GPT2LMHeadModel, AutoTokenizer
from transformers import TrainerCallback
import numpy as np
import torch

val_dataset = load_dataset("roneneldan/TinyStories", split="validation[:1000]")
val_stories = val_dataset["text"]

eval_model = GPT2LMHeadModel.from_pretrained("gpt2")
eval_tokenizer = AutoTokenizer.from_pretrained("gpt2")
eval_tokenizer.pad_token = eval_tokenizer.eos_token

eval_model.eval()
eval_model.to("cuda")

total_perplexity = 0
num_samples = 0
all_perplexities = []
for idx, story in enumerate(val_stories[:100]):
    print(f"Processing story {idx} of 100")
    with torch.no_grad():
        # Get the generated text
        
        # Evaluate perplexity using pretrained GPT-2
        eval_inputs = eval_tokenizer(story, return_tensors="pt", truncation=True, max_length=1024).to("cuda")
        
        # Get model outputs for perplexity calculation
        eval_outputs = eval_model(**eval_inputs, labels=eval_inputs.input_ids)
        loss = eval_outputs.loss
        
        # Calculate perplexity (exp of loss)
        perplexity = torch.exp(loss).item()
        all_perplexities.append(perplexity)
        total_perplexity += perplexity
        num_samples += 1

# Calculate average perplexity
avg_perplexity = total_perplexity / num_samples
print("mean", np.array(all_perplexities).mean(), "std", np.array(all_perplexities).std())
print(f"Average Generative Perplexity (GPT-2 eval) at Step : {avg_perplexity:.4f}")


# [23.14647102355957, 8.609566688537598, 9.647706985473633, 9.277782440185547, 11.145414352416992, 10.916852951049805, 11.68351936340332, 14.344305038452148, 11.121355056762695, 10.245064735412598, 10.617193222045898, 9.528072357177734, 10.275726318359375, 11.375472068786621, 10.76069450378418, 10.554420471191406, 11.763165473937988, 10.411247253417969, 12.114867210388184, 8.355574607849121, 9.977095603942871, 11.770882606506348, 12.58547306060791, 9.674393653869629, 12.341267585754395, 10.044811248779297, 7.8058857917785645, 16.163827896118164, 11.012533187866211, 11.11706256866455, 10.338909149169922, 12.298293113708496, 9.225165367126465, 10.779675483703613, 10.997562408447266, 13.506397247314453, 11.57353401184082, 10.386665344238281, 7.939743995666504, 8.22860050201416, 12.693833351135254, 8.327718734741211, 10.482853889465332, 14.385882377624512, 9.147065162658691, 11.890774726867676, 11.429472923278809, 9.289539337158203, 12.106023788452148, 7.8682146072387695, 7.598713397979736, 6.306299686431885, 10.122062683105469, 9.563165664672852, 10.433128356933594, 10.582348823547363, 11.916589736938477, 8.0909423828125, 8.117325782775879, 8.192804336547852, 11.82377815246582, 11.100614547729492, 12.791665077209473, 8.507798194885254, 9.954484939575195, 23.69622230529785, 12.034829139709473, 13.868413925170898, 13.27900218963623, 12.91967487335205, 13.08671760559082, 13.083039283752441, 7.182500839233398, 8.719032287597656, 8.849884986877441, 10.966265678405762, 10.108570098876953, 9.70151138305664, 10.699834823608398, 12.118654251098633, 9.35136604309082, 11.594557762145996, 11.486559867858887, 11.095831871032715, 9.106569290161133, 13.703754425048828, 9.666036605834961, 11.543903350830078, 10.56528091430664, 11.322389602661133, 7.142597198486328, 8.953615188598633, 8.343801498413086, 8.183747291564941, 13.815485954284668, 11.866976737976074, 12.83556842803955, 8.573541641235352, 10.700776100158691, 11.824569702148438]
# Average Generative Perplexity (GPT-2 eval) at Step : 10.8838
